# Speech Model Compression Papers

This repo presents a collection of recent papers related to speech model compression. Please feel free to suggest other papers!

## 2023

- [[INTERSPEECH]()] [[arXiv](https://arxiv.org/abs/2306.01385)] Task-Agnostic Structured Pruning of Speech Representation Models
- [[INTERSPEECH]()] [[arXiv](https://arxiv.org/abs/2305.17651)] [[code](https://github.com/pyf98/DPHuBERT)] DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
- [[ICASSP](https://ieeexplore.ieee.org/document/10095780)] [[arXiv](https://arxiv.org/abs/2302.14132)] Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding
- [[ICASSP](https://ieeexplore.ieee.org/document/10095480)] [[arXiv](https://arxiv.org/abs/2302.09437)] RobustDistiller: Compressing Universal Speech Representations for Enhanced Environment Robustness
- [[ICASSP](https://ieeexplore.ieee.org/abstract/document/10096445)] [[arXiv](https://arxiv.org/abs/2302.12757)] Ensemble Knowledge Distillation of Self-Supervised Speech Models


## 2022

- [[SLT](https://ieeexplore.ieee.org/document/10022446)] [[arXiv](https://arxiv.org/abs/2207.11906)] Learning a Dual-Mode Speech Recognition Model via Self-Pruning
- [[INTERSPEECH](https://www.isca-speech.org/archive/interspeech_2022/wang22t_interspeech)] [[arXiv](https://arxiv.org/abs/2203.15610)] [[code](https://github.com/mechanicalsea/lighthubert)] LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT
- [[INTERSPEECH](https://www.isca-speech.org/archive/interspeech_2022/ashihara22_interspeech.html)] [[arXiv](https://arxiv.org/abs/2207.06867)] Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models
- [[INTERSPEECH](https://www.isca-speech.org/archive/interspeech_2022/lee22p_interspeech.html)] [[arXiv](https://arxiv.org/abs/2207.00555)] [[code](https://github.com/glory20h/FitHuBERT)] FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning
- [[ICASSP](https://ieeexplore.ieee.org/document/9747490)] [[arXiv](https://arxiv.org/abs/2110.01900)] [[code](https://github.com/s3prl/s3prl/tree/main/s3prl/upstream/distiller)] DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT 

## 2021

- [[NeurIPS](https://proceedings.neurips.cc/paper/2021/hash/b17c0907e67d868b4e0feb43dbbe6f11-Abstract.html)] [[arXiv](https://arxiv.org/abs/2106.05933)] PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition
